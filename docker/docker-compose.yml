x-db-config: &db-config
  DATABASE_HOST: ${DATABASE_HOST}
  DATABASE_PORT: ${DATABASE_PORT}
  DATABASE_USERNAME: ${DATABASE_USERNAME}
  DATABASE_PASSWORD: ${DATABASE_PASSWORD}
  DATABASE_NAME: ${DATABASE_NAME}

x-db-pool-config: &db-pool-config
  DB_MAX_CONNECTIONS: ${DB_MAX_CONNECTIONS:-10}
  DB_ACQUIRE_TIMEOUT_SECONDS: ${DB_ACQUIRE_TIMEOUT_SECONDS:-3}

x-redis-config: &redis-config
  REDIS_URL: ${REDIS_URL}

x-otel-config: &otel-config
  OTEL_EXPORTER_OTLP_ENDPOINT: ${OTEL_EXPORTER_OTLP_ENDPOINT}
  OTEL_DEPLOYMENT_ID: ${OTEL_DEPLOYMENT_ID}
  OTEL_DEPLOYMENT_ENVIRONMENT: ${OTEL_DEPLOYMENT_ENVIRONMENT:-development}
  SERVICE_VERSION: ${SERVICE_VERSION:-0.1.0}

x-storage-config: &storage-config
  STORAGE_BACKEND: ${STORAGE_BACKEND}
  # Only required if STORAGE_BACKEND=s3
  S3_BUCKET: ${S3_BUCKET}
  S3_REGION: ${S3_REGION}

x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "100m"
    max-file: "10"
    compress: "true"

services:
  postgres:
    image: paradedb/paradedb:0.20.6-pg17
    shm_size: 4gb
    container_name: omni-postgres
    environment:
      POSTGRES_DB: ${DATABASE_NAME}
      POSTGRES_USER: ${DATABASE_USERNAME}
      POSTGRES_PASSWORD: ${DATABASE_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - omni-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DATABASE_USERNAME} -d ${DATABASE_NAME}"]
      interval: 10s
      timeout: 5s
      retries: 5
    logging: *default-logging

  redis:
    image: redis:7-alpine
    container_name: omni-redis
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    networks:
      - omni-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    logging: *default-logging

  migrator:
    image: ghcr.io/getomnico/omni/omni-migrator:latest
    container_name: omni-migrator
    environment:
      <<: *db-config
    networks:
      - omni-network
    depends_on:
      postgres:
        condition: service_healthy
    restart: "no"
    logging: *default-logging

  # Core Services
  searcher:
    image: ghcr.io/getomnico/omni/omni-searcher:latest
    container_name: omni-searcher
    expose:
      - "${SEARCHER_PORT}"
    environment:
      <<: [*db-config, *db-pool-config, *redis-config, *otel-config, *storage-config]
      RUST_LOG: ${RUST_LOG}
      PORT: ${SEARCHER_PORT}
      AI_SERVICE_URL: ${AI_SERVICE_URL}
      SEMANTIC_SEARCH_TIMEOUT_MS: ${SEMANTIC_SEARCH_TIMEOUT_MS}
      TYPO_TOLERANCE_ENABLED: ${TYPO_TOLERANCE_ENABLED:-true}
      TYPO_TOLERANCE_MAX_DISTANCE: ${TYPO_TOLERANCE_MAX_DISTANCE:-2}
      TYPO_TOLERANCE_MIN_WORD_LENGTH: ${TYPO_TOLERANCE_MIN_WORD_LENGTH:-4}
    networks:
      - omni-network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      migrator:
        condition: service_completed_successfully
    restart: unless-stopped
    logging: *default-logging

  indexer:
    image: ghcr.io/getomnico/omni/omni-indexer:latest
    container_name: omni-indexer
    expose:
      - "${INDEXER_PORT}"
    environment:
      <<: [*db-config, *db-pool-config, *redis-config, *otel-config, *storage-config]
      RUST_LOG: ${RUST_LOG}
      PORT: ${INDEXER_PORT}
      AI_SERVICE_URL: ${AI_SERVICE_URL}
      ENCRYPTION_KEY: ${ENCRYPTION_KEY}
      ENCRYPTION_SALT: ${ENCRYPTION_SALT}
    networks:
      - omni-network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      migrator:
        condition: service_completed_successfully
    restart: unless-stopped
    logging: *default-logging

  ai:
    image: ghcr.io/getomnico/omni/omni-ai:latest
    container_name: omni-ai
    expose:
      - "${AI_SERVICE_PORT}"
    environment:
      <<: [*db-config, *db-pool-config, *redis-config, *otel-config, *storage-config]
      PORT: ${AI_SERVICE_PORT}
      MODEL_PATH: ${MODEL_PATH}
      EMBEDDING_MODEL: ${EMBEDDING_MODEL}
      EMBEDDING_DIMENSIONS: ${EMBEDDING_DIMENSIONS}
      LLM_PROVIDER: ${LLM_PROVIDER:-vllm}
      VLLM_URL: ${VLLM_URL:-http://vllm:8000}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      ANTHROPIC_MODEL: ${ANTHROPIC_MODEL:-claude-3-5-sonnet-20241022}
      ANTHROPIC_MAX_TOKENS: ${ANTHROPIC_MAX_TOKENS:-4096}
      CUDA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-0}
      AI_WORKERS: ${AI_WORKERS:-1}
      JINA_API_KEY: ${JINA_API_KEY}
      EMBEDDING_PROVIDER: ${EMBEDDING_PROVIDER}
      SEARCHER_URL: ${SEARCHER_URL}
      ENABLE_EMBEDDING_BATCH_INFERENCE: ${ENABLE_EMBEDDING_BATCH_INFERENCE:-false}
      EMBEDDING_BATCH_S3_BUCKET: ${EMBEDDING_BATCH_S3_BUCKET}
      EMBEDDING_BATCH_BEDROCK_ROLE_ARN: ${EMBEDDING_BATCH_BEDROCK_ROLE_ARN}
      EMBEDDING_BATCH_MIN_DOCUMENTS: ${EMBEDDING_BATCH_MIN_DOCUMENTS:-100}
      EMBEDDING_BATCH_MAX_DOCUMENTS: ${EMBEDDING_BATCH_MAX_DOCUMENTS:-50000}
      EMBEDDING_BATCH_ACCUMULATION_TIMEOUT_SECONDS: ${EMBEDDING_BATCH_ACCUMULATION_TIMEOUT_SECONDS:-300}
      EMBEDDING_BATCH_ACCUMULATION_POLL_INTERVAL: ${EMBEDDING_BATCH_ACCUMULATION_POLL_INTERVAL:-10}
      EMBEDDING_BATCH_MONITOR_POLL_INTERVAL: ${EMBEDDING_BATCH_MONITOR_POLL_INTERVAL:-30}
      TITLE_GENERATION_MODEL_ID: ${TITLE_GENERATION_MODEL_ID}
      LOCAL_EMBEDDINGS_URL: ${LOCAL_EMBEDDINGS_URL}
      LOCAL_EMBEDDINGS_MODEL: ${LOCAL_EMBEDDINGS_MODEL}
      OPENAI_EMBEDDING_API_KEY: ${OPENAI_EMBEDDING_API_KEY}
      OPENAI_EMBEDDING_MODEL: ${OPENAI_EMBEDDING_MODEL}
      OPENAI_EMBEDDING_DIMENSIONS: ${OPENAI_EMBEDDING_DIMENSIONS}
    networks:
      - omni-network
    volumes:
      - ai_models:/models
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      migrator:
        condition: service_completed_successfully
    restart: unless-stopped
    logging: *default-logging

  # Connector Manager - Orchestrates all connectors
  connector-manager:
    image: ghcr.io/getomnico/omni/omni-connector-manager:latest
    container_name: omni-connector-manager
    expose:
      - "${CONNECTOR_MANAGER_PORT}"
    environment:
      <<: [*db-config, *db-pool-config, *redis-config, *otel-config]
      RUST_LOG: ${RUST_LOG}
      PORT: ${CONNECTOR_MANAGER_PORT}
      ENCRYPTION_KEY: ${ENCRYPTION_KEY}
      ENCRYPTION_SALT: ${ENCRYPTION_SALT}
      CONNECTOR_GOOGLE_URL: http://google-connector:${GOOGLE_CONNECTOR_PORT}
      CONNECTOR_SLACK_URL: http://slack-connector:${SLACK_CONNECTOR_PORT}
      CONNECTOR_ATLASSIAN_URL: http://atlassian-connector:${ATLASSIAN_CONNECTOR_PORT}
      CONNECTOR_WEB_URL: http://web-connector:${WEB_CONNECTOR_PORT}
      MAX_CONCURRENT_SYNCS: ${MAX_CONCURRENT_SYNCS:-10}
      MAX_CONCURRENT_SYNCS_PER_TYPE: ${MAX_CONCURRENT_SYNCS_PER_TYPE:-3}
      SCHEDULER_POLL_INTERVAL_SECONDS: ${SCHEDULER_POLL_INTERVAL_SECONDS:-60}
      STALE_SYNC_TIMEOUT_MINUTES: ${STALE_SYNC_TIMEOUT_MINUTES:-10}
    networks:
      - omni-network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      migrator:
        condition: service_completed_successfully
    restart: unless-stopped
    logging: *default-logging

  # vLLM Service for LLM inference (optional - only when using vLLM provider)
  # GPU support: use docker-compose.gpu.yml override for GPU acceleration
  vllm:
    image: vllm/vllm-openai:latest
    container_name: omni-vllm
    expose:
      - "${VLLM_PORT}"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
    command: >
      --model ${VLLM_MODEL}
      --dtype ${VLLM_DTYPE:-auto}
      --api-key ${VLLM_API_KEY}
      --port ${VLLM_PORT}
      --host 0.0.0.0
      --max-model-len ${VLLM_MAX_MODEL_LEN}
    networks:
      - omni-network
    volumes:
      - vllm_models:/root/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - vllm  # Only start when vllm profile is active
    logging: *default-logging

  # vLLM Embeddings Service (for local embedding models)
  # GPU support: use docker-compose.gpu.yml override for GPU acceleration
  embeddings:
    image: vllm/vllm-openai:latest
    container_name: omni-embeddings
    expose:
      - "${VLLM_EMBEDDINGS_PORT}"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}
    command:
      - "--model"
      - "${LOCAL_EMBEDDINGS_MODEL}"
      - "--dtype"
      - "${VLLM_DTYPE}"
      - "--port"
      - "${VLLM_EMBEDDINGS_PORT}"
      - "--host"
      - "0.0.0.0"
      - "--max-model-len"
      - "${VLLM_EMBEDDINGS_MAX_MODEL_LEN}"
      - "--trust-remote-code"
    networks:
      - omni-network
    volumes:
      - vllm_models:/root/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${VLLM_EMBEDDINGS_PORT}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - local-embeddings  # Only start when local-embeddings profile is active
    logging: *default-logging

  # Connector Services
  google-connector:
    image: ghcr.io/getomnico/omni/omni-google-connector:latest
    container_name: omni-google-connector
    expose:
      - "${GOOGLE_CONNECTOR_PORT}"
    environment:
      <<: [*redis-config, *otel-config]
      RUST_LOG: ${RUST_LOG}
      PORT: ${GOOGLE_CONNECTOR_PORT}
      AI_SERVICE_URL: ${AI_SERVICE_URL}
      CONNECTOR_MANAGER_URL: http://connector-manager:${CONNECTOR_MANAGER_PORT}
      GOOGLE_SYNC_INTERVAL_SECONDS: ${GOOGLE_SYNC_INTERVAL_SECONDS}
      GOOGLE_MAX_AGE_DAYS: ${GOOGLE_MAX_AGE_DAYS:-730}
      GOOGLE_WEBHOOK_URL: ${GOOGLE_WEBHOOK_URL}
      WEBHOOK_RENEWAL_CHECK_INTERVAL_SECONDS: ${WEBHOOK_RENEWAL_CHECK_INTERVAL_SECONDS:-3600}
      ENCRYPTION_KEY: ${ENCRYPTION_KEY}
      ENCRYPTION_SALT: ${ENCRYPTION_SALT}
    networks:
      - omni-network
    depends_on:
      redis:
        condition: service_healthy
      connector-manager:
        condition: service_started
    restart: unless-stopped
    logging: *default-logging

  slack-connector:
    image: ghcr.io/getomnico/omni/omni-slack-connector:latest
    container_name: omni-slack-connector
    expose:
      - "${SLACK_CONNECTOR_PORT}"
    environment:
      <<: [*redis-config, *otel-config]
      RUST_LOG: ${RUST_LOG}
      PORT: ${SLACK_CONNECTOR_PORT}
      SLACK_BOT_TOKEN: ${SLACK_BOT_TOKEN}
      CONNECTOR_MANAGER_URL: http://connector-manager:${CONNECTOR_MANAGER_PORT}
    networks:
      - omni-network
    depends_on:
      redis:
        condition: service_healthy
      connector-manager:
        condition: service_started
    restart: unless-stopped
    logging: *default-logging

  atlassian-connector:
    image: ghcr.io/getomnico/omni/omni-atlassian-connector:latest
    container_name: omni-atlassian-connector
    expose:
      - "${ATLASSIAN_CONNECTOR_PORT}"
    environment:
      <<: [*redis-config, *otel-config]
      RUST_LOG: ${RUST_LOG}
      PORT: ${ATLASSIAN_CONNECTOR_PORT}
      CONNECTOR_MANAGER_URL: http://connector-manager:${CONNECTOR_MANAGER_PORT}
      ATLASSIAN_BASE_URL: ${ATLASSIAN_BASE_URL}
      ATLASSIAN_USER_EMAIL: ${ATLASSIAN_USER_EMAIL}
      ATLASSIAN_API_TOKEN: ${ATLASSIAN_API_TOKEN}
      ENCRYPTION_KEY: ${ENCRYPTION_KEY}
      ENCRYPTION_SALT: ${ENCRYPTION_SALT}
    networks:
      - omni-network
    depends_on:
      redis:
        condition: service_healthy
      connector-manager:
        condition: service_started
    restart: unless-stopped
    logging: *default-logging

  web-connector:
    image: ghcr.io/getomnico/omni/omni-web-connector:latest
    container_name: omni-web-connector
    expose:
      - "${WEB_CONNECTOR_PORT}"
    environment:
      <<: [*redis-config, *otel-config]
      RUST_LOG: ${RUST_LOG}
      PORT: ${WEB_CONNECTOR_PORT}
      CONNECTOR_MANAGER_URL: http://connector-manager:${CONNECTOR_MANAGER_PORT}
    networks:
      - omni-network
    depends_on:
      connector-manager:
        condition: service_started
      redis:
        condition: service_healthy
    restart: unless-stopped
    logging: *default-logging

  # Web Application (SvelteKit frontend + backend)
  web:
    image: ghcr.io/getomnico/omni/omni-web:latest
    container_name: omni-web
    expose:
      - "${WEB_PORT}"
    environment:
      <<: [*db-config, *db-pool-config, *redis-config, *otel-config]
      SEARCHER_URL: ${SEARCHER_URL}
      INDEXER_URL: ${INDEXER_URL}
      AI_SERVICE_URL: ${AI_SERVICE_URL}
      CONNECTOR_MANAGER_URL: ${CONNECTOR_MANAGER_URL}
      GOOGLE_CONNECTOR_URL: ${GOOGLE_CONNECTOR_URL}
      SLACK_CONNECTOR_URL: ${SLACK_CONNECTOR_URL}
      ATLASSIAN_CONNECTOR_URL: ${ATLASSIAN_CONNECTOR_URL}
      WEB_CONNECTOR_URL: ${WEB_CONNECTOR_URL}
      SESSION_SECRET: ${SESSION_SECRET}
      SESSION_COOKIE_NAME: ${SESSION_COOKIE_NAME}
      SESSION_DURATION_DAYS: ${SESSION_DURATION_DAYS}
      APP_URL: ${APP_URL}
      EMAIL_PROVIDER: ${EMAIL_PROVIDER:-resend}
      RESEND_API_KEY: ${RESEND_API_KEY}
      EMAIL_FROM: ${EMAIL_FROM:-Clio <noreply@yourdomain.com>}
      EMAIL_HOST: ${EMAIL_HOST}
      EMAIL_PORT: ${EMAIL_PORT}
      EMAIL_USER: ${EMAIL_USER}
      EMAIL_PASSWORD: ${EMAIL_PASSWORD}
      EMAIL_SECURE: ${EMAIL_SECURE}
      AI_ANSWER_ENABLED: ${AI_ANSWER_ENABLED}
      AI_FIRST_SEARCH_ENABLED: ${AI_FIRST_SEARCH_ENABLED}
    networks:
      - omni-network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      migrator:
        condition: service_completed_successfully
      searcher:
        condition: service_started
      indexer:
        condition: service_started
      ai:
        condition: service_started
    restart: unless-stopped
    logging: *default-logging

  # Load Balancer / Reverse Proxy
  caddy:
    image: caddy:2-alpine
    container_name: omni-caddy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ../Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config
    environment:
      OMNI_DOMAIN: ${OMNI_DOMAIN}
      ACME_EMAIL: ${ACME_EMAIL}
      WEB_PORT: ${WEB_PORT}
      GOOGLE_CONNECTOR_PORT: ${GOOGLE_CONNECTOR_PORT}
      SLACK_CONNECTOR_PORT: ${SLACK_CONNECTOR_PORT}
      ATLASSIAN_CONNECTOR_PORT: ${ATLASSIAN_CONNECTOR_PORT}
    networks:
      - omni-network
    depends_on:
      - web
    restart: unless-stopped
    logging: *default-logging

networks:
  omni-network:
    driver: bridge

volumes:
  postgres_data:
    name: omni-postgres-data
  redis_data:
    name: omni-redis-data
  ai_models:
    name: omni-ai-models
  vllm_models:
    name: omni-vllm-models
  caddy_data:
    name: omni-caddy-data
  caddy_config:
    name: omni-caddy-config
